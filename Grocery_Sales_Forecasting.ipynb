{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sultanmr/retail_demand_forecast/blob/main/Grocery_Sales_Forecasting.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1: Initialize Libraries"
      ],
      "metadata": {
        "id": "oRKiahg5LFsB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import matplotlib.pyplot as plt\n",
        "import gdown\n",
        "import pickle\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "import xgboost as xgb\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "required_packages = [\"xgboost\", \"tensorflow\", \"mlflow\", \"pyngrok\"]\n",
        "\n",
        "for package in required_packages:\n",
        "    try:\n",
        "        __import__(package)\n",
        "    except ImportError:\n",
        "        print(f\"{package} not found, installing now...\")\n",
        "        !pip install {package}\n"
      ],
      "metadata": {
        "id": "bpL5kuPjLJSv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2: Loading Data Faster"
      ],
      "metadata": {
        "id": "tnaMqCSrkOhy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "holidays_events_url = \"https://docs.google.com/spreadsheets/d/e/2PACX-1vQFI6-mQt7_iVBbY8XgYrv1Y0Qq-cVjY91K-N0CXGxVPDFZ0Vp41x5kyRoaEcGB836cjXvbT5zLB0nj/pub?gid=446609883&single=true&output=csv\"\n",
        "stores_url = \"https://docs.google.com/spreadsheets/d/e/2PACX-1vSS4Jk3_oWrRikXPVa73ZbSk48j4XN2NuyHij8HkH_68Ma_xmwLn-Omzmb_ka35vYmYp4gawr4LgygC/pub?gid=1518879967&single=true&output=csv\"\n",
        "items_url = \"https://docs.google.com/spreadsheets/d/e/2PACX-1vSMtQyDxmnC1bIiaznXtquMxYEe0A1rFin-CBFh2SZd2C7Tm9Qr8QxGbh1cI6XprZZ-2TTNv1oNsO_p/pub?gid=322268302&single=true&output=csv\"\n",
        "transactions_url = \"https://docs.google.com/spreadsheets/d/e/2PACX-1vQc3XGa4BZLwERgPXpykRX1XZjrX0MM-53xz-v17AycH-a-KNV1T_ZyNB-PQJBE0Ho6Z-Lr2k11HQNY/pub?gid=1224454227&single=true&output=csv\"\n",
        "oil_url = \"https://docs.google.com/spreadsheets/d/e/2PACX-1vRamDmxeiATZkEI2Ywe5kGisXO4GXi7RWcE8a31MUpSzXJuOehZeb2RdKoEhO5ZEu8okaTebH4rQVWf/pub?gid=310218469&single=true&output=csv\"\n",
        "\n",
        "df_holidays_events = pd.read_csv(holidays_events_url)\n",
        "df_stores = pd.read_csv(stores_url)\n",
        "df_items = pd.read_csv(items_url)\n",
        "df_transactions = pd.read_csv(transactions_url)\n",
        "df_oil = pd.read_csv(oil_url)"
      ],
      "metadata": {
        "id": "KfKBdzj4jcRy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_url = \"https://drive.google.com/uc?id=1BUcG6vUSAmduBQS3_VlUsdQB_mQ_zdn2\"\n",
        "gdown.download(train_url, \"train.csv\", quiet=True)\n",
        "df_train = pd.read_csv(\"train.csv\")"
      ],
      "metadata": {
        "id": "0-4B44QY2BuZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train.head()"
      ],
      "metadata": {
        "id": "DbBzMXo7Qh3s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N6oZvC41fodM"
      },
      "source": [
        "# Step 2: Loading All the Data from Google Drive\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4KZfFQNiOWsg"
      },
      "outputs": [],
      "source": [
        "if len(df_oil)==0:\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive')\n",
        "\n",
        "  root_path = \"/content/drive/MyDrive/ML/datasets/retail_kaggle_data/\"\n",
        "  # Load the CSV files into pandas DataFrames\n",
        "  df_stores = pd.read_csv(root_path+'stores.csv')\n",
        "  df_items = pd.read_csv(root_path+'items.csv')\n",
        "  df_transactions = pd.read_csv(root_path+'transactions.csv')\n",
        "  df_oil = pd.read_csv(root_path+'oil.csv')\n",
        "  df_holidays_events = pd.read_csv(root_path+'holidays_events.csv')\n",
        "  # Select list of stores located in the 'Pichincha' region\n",
        "  store_ids = df_stores[df_stores['state'] == 'Pichincha']['store_nbr'].unique()\n",
        "\n",
        "  # Initialize an empty list to hold filtered chunks\n",
        "  filtered_chunks = []\n",
        "\n",
        "  # Define the chunk size (number of rows per chunk)\n",
        "  chunk_size = 10 ** 6  # Adjust based on your system's memory capacity\n",
        "\n",
        "  # Read the CSV file in chunks\n",
        "  for chunk in pd.read_csv(root_path+'train.csv', chunksize=chunk_size):\n",
        "      # Filter the chunk for the desired store IDs\n",
        "      chunk_filtered = chunk[chunk['store_nbr'].isin(store_ids)]\n",
        "      # Append the filtered chunk to the list\n",
        "      filtered_chunks.append(chunk_filtered)\n",
        "      # Optional: Delete the chunk to free up memory\n",
        "      del chunk\n",
        "\n",
        "  # Concatenate all filtered chunks into a single DataFrame\n",
        "  df_train = pd.concat(filtered_chunks, ignore_index=True)\n",
        "\n",
        "  # Clean up to free memory\n",
        "  del filtered_chunks\n",
        "\n",
        "  #Filter out train.csv file even further by selecting only the rows with the dates that are before April'14. This is what we did in the lessons of the sprint and again need to do here in the project.\n",
        "  max_date = '2014-04-01'\n",
        "  df_train = df_train[(df_train['date'] < max_date)]\n",
        "\n",
        "  df_train.to_csv('train_filtered.csv', index=False, header=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7dvKtNmLfwtP"
      },
      "source": [
        "# Step 2: Checking for Missing Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TA7GWcFvV79c"
      },
      "outputs": [],
      "source": [
        "# Checking missing values\n",
        "df_train.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dBUFKepwWCL8"
      },
      "outputs": [],
      "source": [
        "# Focusing on missing values in the 'onpromotion' column\n",
        "df_train['onpromotion'] = df_train['onpromotion'].fillna(False).astype(bool)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bnMFYVIvf3zf"
      },
      "source": [
        "# Step 3: Handling Outliers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cdTHQRRUWJYU"
      },
      "outputs": [],
      "source": [
        "# Checking for negative sales (returns)\n",
        "negative_sales = df_train[df_train['unit_sales'] < 0]\n",
        "\n",
        "# Replacing negative sales with 0 to reflect returns as non-sales\n",
        "df_train['unit_sales'] = df_train['unit_sales'].apply(lambda x: max(x, 0))\n",
        "\n",
        "negative_sales.head()  # Viewing negative sales for analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S0Kd2l6PcAkh"
      },
      "outputs": [],
      "source": [
        "df_train['z_score'] = df_train.groupby(['store_nbr', 'item_nbr'])['unit_sales'].transform(\n",
        "    lambda x: (x - x.mean()) / (x.std() if x.std() != 0 else 1)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MJLCA7xPcUv1"
      },
      "outputs": [],
      "source": [
        "outliers = df_train[df_train['z_score'] > 5]\n",
        "# Print summary\n",
        "print(f\"Number of outliers detected: {len(outliers)}\")\n",
        "outliers.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1pR5vMUSf_QJ"
      },
      "source": [
        "# Step 4: Fill missing dates with zero sales"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bQxI7menbA6i"
      },
      "outputs": [],
      "source": [
        "# Convert 'date' column to datetime format\n",
        "df_train['date'] = pd.to_datetime(df_train['date'])\n",
        "\n",
        "# Get the minimum and maximum dates in the dataset to create a full date range\n",
        "min_date = df_train['date'].min()\n",
        "max_date = df_train['date'].max()\n",
        "\n",
        "# Get full date range\n",
        "full_date_range = pd.DataFrame({'date': pd.date_range(start=min_date, end=max_date, freq='D')})\n",
        "\n",
        "\n",
        "# Create a DataFrame with all (store, item, date) combinations\n",
        "store_item_combinations = df_train[['store_nbr', 'item_nbr']].drop_duplicates()\n",
        "all_combinations = store_item_combinations.merge(full_date_range, how='cross')\n",
        "\n",
        "# Merge with original data to fill missing dates\n",
        "df_filled = all_combinations.merge(df_train, on=['store_nbr', 'item_nbr', 'date'], how='left')\n",
        "\n",
        "# Fill missing sales values with 0\n",
        "df_filled['unit_sales'] = df_filled['unit_sales'].fillna(0)\n",
        "\n",
        "# Check the first few rows\n",
        "print(df_filled.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jKQXwPO2gMOD"
      },
      "source": [
        "# Step 5: Feature Engineering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PDBT8gqegPJ7"
      },
      "outputs": [],
      "source": [
        "# Convert date column to datetime\n",
        "df_train['date'] = pd.to_datetime(df_train['date'])\n",
        "\n",
        "# Creating new time-based features\n",
        "df_train['year'] = df_train['date'].dt.year\n",
        "df_train['month'] = df_train['date'].dt.month\n",
        "df_train['day'] = df_train['date'].dt.day\n",
        "df_train['day_of_week'] = df_train['date'].dt.dayofweek"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m2x-ja-ue2kL"
      },
      "outputs": [],
      "source": [
        "# Calculating rolling average of unit_sales\n",
        "df_train['unit_sales_7d_avg'] = df_train.groupby(['item_nbr', 'store_nbr'])['unit_sales'].transform(lambda x: x.rolling(window=7).mean())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MsaAnYUCgUD-"
      },
      "source": [
        "# Step 6: Visualizing Time-Series Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oXiuoSGhfDMn"
      },
      "source": [
        "# a) Sales Over Time (Aggregated)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yzax_bdWe6Bc"
      },
      "outputs": [],
      "source": [
        "# Aggregating total sales by date\n",
        "sales_by_date = df_train.groupby('date')['unit_sales'].sum()\n",
        "\n",
        "# Plotting the time-series\n",
        "plt.figure(figsize=(12,6))\n",
        "plt.plot(sales_by_date.index, sales_by_date.values)\n",
        "plt.title('Total Unit Sales Over Time in Pichincha state', fontsize=20, fontweight='bold')\n",
        "plt.xlabel('Date', fontsize=16)\n",
        "plt.ylabel('Unit Sales', fontsize=16)\n",
        "plt.xticks(fontsize=14, rotation=45)\n",
        "plt.yticks(fontsize=14)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "efQuFLgKfIiq"
      },
      "source": [
        "# b) Sales Trend by Year and Month"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KBdrM_tdfKY-"
      },
      "outputs": [],
      "source": [
        "# Aggregating sales by year and month\n",
        "sales_by_month = df_train.groupby(['year', 'month'])['unit_sales'].sum().unstack()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2k3qDixEfLtG"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "plt.figure(figsize=(8, 5))  # Increase figure size for better visibility\n",
        "sns.heatmap(\n",
        "    sales_by_month,\n",
        "    cmap='coolwarm',  # Use a diverging colormap for better contrast\n",
        "    linewidths=0.5,  # Add lines between cells for clarity\n",
        "    linecolor='white',  # Use white lines for a cleaner look\n",
        "    cbar_kws={'label': 'Sales Volume'}  # Add a descriptive colorbar label\n",
        ")\n",
        "\n",
        "# Customizing title and axes labels\n",
        "plt.title('Monthly Sales Trends Over Years', fontsize=22, fontweight='bold')\n",
        "plt.xlabel('Month', fontsize=18, labelpad=10)  # Labelpad adds spacing\n",
        "plt.ylabel('Year', fontsize=18, labelpad=10)\n",
        "\n",
        "# Formatting tick labels\n",
        "plt.xticks(fontsize=14, rotation=45)  # Rotate x-axis labels for better readability\n",
        "plt.yticks(fontsize=14)\n",
        "\n",
        "# Adjust layout for better spacing\n",
        "plt.tight_layout()\n",
        "\n",
        "# Display the heatmap\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ewwMefvmfOxf"
      },
      "source": [
        "# Step 7: Examining the Impact of Holidays"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ySezDinfQzq"
      },
      "outputs": [],
      "source": [
        "# Convert date column to datetime\n",
        "df_holidays_events['date'] = pd.to_datetime(df_holidays_events['date'])\n",
        "print(\"Holidays range: from\",df_holidays_events['date'].dt.date.min(),\"till\",df_holidays_events['date'].dt.date.max())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q1XenChyfRuq"
      },
      "outputs": [],
      "source": [
        "# Merging df_train data with holidays\n",
        "df_train_holiday = pd.merge(df_train, df_holidays_events, on='date', how='left')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N9lbjcRMfSwL"
      },
      "outputs": [],
      "source": [
        "# Aggregating sales by holiday and non-holiday\n",
        "holiday_sales = df_train_holiday.groupby('type')['unit_sales'].mean()\n",
        "\n",
        "# Plotting holiday impact\n",
        "plt.figure(figsize=(8,5))\n",
        "holiday_sales.plot(kind='bar', color='lightgreen', edgecolor='black')\n",
        "plt.title('Impact of Holidays on Sales', fontsize=20, fontweight='bold')\n",
        "plt.ylabel('Average Unit Sales', fontsize=16)\n",
        "plt.xlabel('')\n",
        "plt.xticks(fontsize=14)\n",
        "plt.yticks(fontsize=14)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sLq_GL45fUxK"
      },
      "source": [
        "# Step 8: Analyzing Perishable Items"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EgmSlYpCfWta"
      },
      "outputs": [],
      "source": [
        "# Merging df_train with items to get perishable data\n",
        "df_train_items = pd.merge(df_train, df_items, on='item_nbr', how='left')\n",
        "df_train_items['perishable'] = df_train_items['perishable'].astype(bool)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z_YteqxQfXw6"
      },
      "outputs": [],
      "source": [
        "# Aggregating sales by perishable and non-perishable items\n",
        "perishable_sales = df_train_items.groupby('perishable')['unit_sales'].sum()\n",
        "\n",
        "# Plotting sales for perishable and non-perishable items\n",
        "plt.figure(figsize=(12,6))\n",
        "perishable_sales.plot(kind='bar', color=['orange', 'green'], edgecolor='black')\n",
        "plt.title('Sales of Perishable vs Non-Perishable Items', fontsize=16)\n",
        "plt.ylabel('Total Unit Sales', fontsize=16)\n",
        "plt.xlabel('')\n",
        "plt.xticks(\n",
        "    ticks=[0, 1],\n",
        "    labels=['Non-Perishable', 'Perishable'],\n",
        "    fontsize=16,\n",
        "    rotation=0  # Keep x-axis labels horizontal\n",
        ")\n",
        "plt.yticks(fontsize=14)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Week 2 Goals!"
      ],
      "metadata": {
        "id": "R8tt30RPw9KA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Top-3 performing item families"
      ],
      "metadata": {
        "id": "ns6_F1ABw9NP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_items['family'].value_counts(sort=False).head(3)"
      ],
      "metadata": {
        "id": "MyVBZ1vZxSu0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# let's have a list that top-3 families in it\n",
        "item_families = ['GROCERY I', 'BEVERAGES', 'CLEANING']\n",
        "\n",
        "# now, we can get ids for the items that are in these familieis\n",
        "item_ids = df_items[df_items['family'].isin(item_families)]['item_nbr'].unique()\n",
        "\n",
        "# next, we read in the train.csv data and\n",
        "# filter it out to get desriesd item ids\n",
        "# here is how we filter\n",
        "df_train = df_train[(df_train['item_nbr'].isin(item_ids))]"
      ],
      "metadata": {
        "id": "aawG2nCyxgcc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Filter out train.csv file even further by selecting only the rows with the dates that are before April'14. This is what we did in the lessons of the sprint and again need to do here in the project.\n",
        "max_date = '2014-04-01'\n",
        "df_train = df_train[(df_train['date'] < max_date)]"
      ],
      "metadata": {
        "id": "AWTHu2jKzmoZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# let's assumne that train_df is the variable that has the data in it\n",
        "# and df_stores is the dataframe that contains data about stores\n",
        "# so you can merge these two dataframes like this:\n",
        "df_train = df_train.merge(df_stores, on='store_nbr', how='left')\n",
        "\n",
        "# let's also assume that df_items is the dataframe that\n",
        "# contains data obout items. So, let's merge now\n",
        "df_train = df_train.merge(df_items, on='item_nbr', how='left')\n"
      ],
      "metadata": {
        "id": "oJ9oQrmO0oYo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train.head()"
      ],
      "metadata": {
        "id": "SZ82MPVl5qng"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Split the dataset into training and testing portions."
      ],
      "metadata": {
        "id": "-hK-oWhgFQv_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sort by date to ensure chronological order\n",
        "df_train = df_train.sort_values(by='date')\n",
        "\n",
        "# Determine the split index (80% for training, 20% for testing)\n",
        "split_index = int(len(df_train) * 0.8)\n",
        "\n",
        "# Split the dataset\n",
        "train_data = df_train.iloc[:split_index]  # First 80%\n",
        "test_data = df_train.iloc[split_index:]   # Last 20%\n"
      ],
      "metadata": {
        "id": "6sUrs25A3IO1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Split the target variable"
      ],
      "metadata": {
        "id": "mh1mfwDMF_7w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Fxk0cbemF_H_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.columns"
      ],
      "metadata": {
        "id": "AOniFAw_FiAz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cols_to_drop = ['id', 'date', 'unit_sales', 'onpromotion', 'store_nbr', 'item_nbr']\n",
        "label_col = 'unit_sales'\n",
        "\n",
        "y_train = train_data[label_col]\n",
        "X_train = train_data.drop(cols_to_drop, axis=1)\n",
        "\n",
        "y_test = test_data[label_col]\n",
        "X_test = test_data.drop(cols_to_drop, axis=1)"
      ],
      "metadata": {
        "id": "fEi9kQfaGJf_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cleaning"
      ],
      "metadata": {
        "id": "5N0wBOl9H2or"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train['unit_sales_7d_avg'].fillna(0, inplace=True)\n",
        "X_test['unit_sales_7d_avg'].fillna(0, inplace=True)\n",
        "\n",
        "categorical_cols = ['city', 'state', 'type', 'family']\n",
        "\n",
        "# Apply Label Encoding to categorical columns\n",
        "label_encoders = {}\n",
        "for col in categorical_cols:\n",
        "    label_encoders[col] = LabelEncoder()\n",
        "    X_train[col] = label_encoders[col].fit_transform(X_train[col])\n",
        "    X_test[col] = label_encoders[col].transform(X_test[col])"
      ],
      "metadata": {
        "id": "h1sY7pH6HsNr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Initialize MLFlow"
      ],
      "metadata": {
        "id": "RpgQTLkM-H0q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok"
      ],
      "metadata": {
        "id": "fVY1DmbEmIEx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "import mlflow\n",
        "ngrok.set_auth_token(\"2uj6ia3aPjGFf0A4arHdWFTU4xl_3HoYKVeNMkSd31veLbVKc\")\n",
        "\n",
        "\n",
        "ngrok.kill()\n",
        "mlflow_storage_path = \"/content/mlruns\"\n",
        "get_ipython().system_raw(\"mlflow server --backend-store-uri {} --host 0.0.0.0 --port 5000 &\".format(mlflow_storage_path))\n",
        "\n",
        "mlflow_url = ngrok.connect(5000).public_url\n",
        "print(\"MLflow Tracking UI:\", mlflow_url)\n",
        "#mlflow.set_tracking_uri(\"http://127.0.0.1:5000\")\n",
        "mlflow.set_tracking_uri(f\"file:{mlflow_storage_path}\")\n",
        "mlflow.set_experiment(\"Grocery Sales Forecasting\")"
      ],
      "metadata": {
        "id": "bRgyouww-KsJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# XGBoost with RandomizedSearchCV"
      ],
      "metadata": {
        "id": "lEj-e6AsGbeX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import xgboost as xgb\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "import mlflow\n",
        "import mlflow.xgboost\n",
        "from mlflow.models import infer_signature\n",
        "\n",
        "param_dist = {\n",
        "    'n_estimators': [100, 200, 300],\n",
        "    'max_depth': [3, 5, 7],\n",
        "    'learning_rate': [0.01, 0.1, 0.2],\n",
        "    'subsample': [0.7, 0.8, 1.0],\n",
        "    'colsample_bytree': [0.7, 0.8, 1.0],\n",
        "    'gamma': [0.0, 0.1, 0.2],\n",
        "    'reg_alpha': [0.001, 0.01, 0.1],\n",
        "    'reg_lambda': [0.001, 0.01, 0.1]\n",
        "}\n",
        "\n",
        "\n",
        "xgb_model = xgb.XGBRegressor(objective='reg:squarederror', random_state=42)\n",
        "\n",
        "random_search = RandomizedSearchCV(\n",
        "    estimator=xgb_model,\n",
        "    param_distributions=param_dist,\n",
        "    n_iter=10,\n",
        "    scoring='neg_mean_squared_error',\n",
        "    cv=3,\n",
        "    verbose=2,\n",
        "    n_jobs=2,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "\n",
        "random_search.fit(X_train, y_train)\n",
        "best_params = random_search.best_params_\n"
      ],
      "metadata": {
        "id": "B-BAHLcy-fYp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Log model and metrics to MLflow"
      ],
      "metadata": {
        "id": "1MPncYpZbQLM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from mlflow.models import infer_signature\n",
        "\n",
        "with mlflow.start_run(run_name=\"XGBoost Model\"):\n",
        "    print(f\"Training with best params: {best_params}\")\n",
        "    #xgb_model = xgb.XGBRegressor(**best_params, objective='reg:squarederror', random_state=42)\n",
        "    #xgb_model.fit(X_train, y_train)\n",
        "    xgb_model = random_search.best_estimator_\n",
        "    y_pred_xgb = xgb_model.predict(X_test)\n",
        "    mse = mean_squared_error(y_test, y_pred_xgb)\n",
        "    rmse = np.sqrt(mse)\n",
        "\n",
        "    mlflow.log_params(best_params)\n",
        "    mlflow.log_metric(\"MSE\", mse)\n",
        "    mlflow.log_metric(\"RMSE\", rmse)\n",
        "   # xgb_model.save(\"best_xgb_model.h5\")\n",
        "\n",
        "    input_example = X_test\n",
        "    signature = infer_signature(X_train, xgb_model.predict(X_train))\n",
        "\n",
        "    mlflow.xgboost.log_model(\n",
        "        xgb_model,\n",
        "        artifact_path= \"XGBoost_Best_Model\",\n",
        "        signature=signature,\n",
        "        input_example=input_example\n",
        "    )\n",
        "\n",
        "    print(f\"Run logged to MLflow - RMSE: {rmse:.4f}\")\n",
        "\n",
        "with open('xgb_model.pkl', 'wb') as file:\n",
        "    pickle.dump(xgb_model, file)\n",
        "\n",
        "\n",
        "print(\"Open MLflow UI here:\", mlflow_url)"
      ],
      "metadata": {
        "id": "J2bVfDz-YzGC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rmse = mean_squared_error(y_test, y_pred_xgb) ** 0.5\n",
        "print(f'Root Mean Squared Error (RMSE): {rmse:.4f}')"
      ],
      "metadata": {
        "id": "FjKajw3aIGvl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data['date'] = pd.to_datetime(test_data['date'])\n",
        "\n",
        "df_results = test_data.copy()\n",
        "df_results['actual_sales'] = y_test\n",
        "df_results['predicted_sales'] = y_pred_xgb\n",
        "\n",
        "df_results = df_results[['date', 'actual_sales', 'predicted_sales']]\n",
        "df_results = df_results.sort_values(by='date')\n",
        "\n",
        "# Aggregate by month\n",
        "df_monthly = df_results.resample('M', on='date').sum()\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(14, 6))\n",
        "sns.set_style(\"whitegrid\")\n",
        "\n",
        "plt.plot(df_monthly.index, df_monthly['actual_sales'], label=\"Actual Sales\", color=\"blue\", marker='o', linestyle='dashed')\n",
        "plt.plot(df_monthly.index, df_monthly['predicted_sales'], label=\"Predicted Sales\", color=\"red\", marker='s', linestyle='dashed')\n",
        "\n",
        "\n",
        "plt.xlabel(\"Date (Monthly)\")\n",
        "plt.ylabel(\"Unit Sales\")\n",
        "plt.title(\"Actual vs. Predicted Unit Sales (Monthly Aggregation)\")\n",
        "plt.xticks(rotation=45)  # Rotate x-axis labels for readability\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "KZpWSkAqJH6V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LSTM"
      ],
      "metadata": {
        "id": "RWXeCq_QJ3Hs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import mlflow\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import ParameterSampler\n",
        "from pyngrok import ngrok\n",
        "from mlflow.models import infer_signature\n",
        "\n",
        "def create_lstm_model(units=50, learning_rate=0.001):\n",
        "    model = Sequential([\n",
        "        LSTM(units, activation='relu', return_sequences=True, input_shape=(X_train_lstm.shape[1], X_train_lstm.shape[2])),\n",
        "        Dense(1)\n",
        "    ])\n",
        "    optimizer = Adam(learning_rate=learning_rate)\n",
        "    model.compile(optimizer=optimizer, loss='mse')\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "lKNRKT1YKI4m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#RELOADING DATA (IN CASE OF CRASH)\n",
        "import os\n",
        "import gdown\n",
        "\n",
        "# Define file name\n",
        "train_file = \"train.csv\"\n",
        "\n",
        "# Check if file exists\n",
        "if not os.path.exists(train_file):\n",
        "    print(\"train.csv not found. Downloading...\")\n",
        "    train_url = \"https://drive.google.com/uc?id=1BUcG6vUSAmduBQS3_VlUsdQB_mQ_zdn2\"\n",
        "    gdown.download(train_url, train_file, quiet=True)\n",
        "    print(\"Download complete.\")\n",
        "else:\n",
        "    print(\"train.csv already exists. Skipping download.\")\n",
        "df_train = pd.read_csv(train_file)\n",
        "\n",
        "df_train = df_train.sort_values(by='date')\n",
        "split_index = int(len(df_train) * 0.8)\n",
        "\n",
        "df_train['date'] = pd.to_datetime(df_train['date'])\n",
        "train_data = df_train.iloc[:split_index]  # First 80%\n",
        "test_data = df_train.iloc[split_index:]   # Last 20%"
      ],
      "metadata": {
        "id": "4UshzHQzu9wl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import numpy as np\n",
        "\n",
        "train_data = train_data.sort_values(by=\"date\")\n",
        "test_data = test_data.sort_values(by=\"date\")\n",
        "\n",
        "scaler = MinMaxScaler(feature_range=(0,1))\n",
        "train_data['unit_sales_scaled'] = scaler.fit_transform(train_data[['unit_sales']])\n",
        "test_data['unit_sales_scaled'] = scaler.transform(test_data[['unit_sales']])\n",
        "\n",
        "def create_sequences(data, seq_length=10):\n",
        "    X, y = [], []\n",
        "    for i in range(len(data) - seq_length):\n",
        "        X.append(data[i:i+seq_length])\n",
        "        y.append(data[i+seq_length])\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "seq_length = 10\n",
        "\n",
        "X_train_lstm, y_train_lstm = create_sequences(train_data['unit_sales_scaled'].values, seq_length)\n",
        "X_test_lstm, y_test_lstm = create_sequences(test_data['unit_sales_scaled'].values, seq_length)\n",
        "\n",
        "X_train_lstm = X_train_lstm.reshape((X_train_lstm.shape[0], X_train_lstm.shape[1], 1))\n",
        "X_test_lstm = X_test_lstm.reshape((X_test_lstm.shape[0], X_test_lstm.shape[1], 1))"
      ],
      "metadata": {
        "id": "cC8Mp40M0AdN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lstm_model = create_lstm_model()\n",
        "lstm_model.fit(X_train_lstm, y_train_lstm,\n",
        "          epochs=1,\n",
        "          batch_size=128,\n",
        "          validation_data=(X_test_lstm, y_test_lstm),\n",
        "          verbose=1)\n",
        "\n",
        "# Predict test values\n",
        "y_pred_lstm = lstm_model.predict(X_test_lstm)\n",
        "\n",
        "# Reverse scaling\n",
        "y_pred_lstm = scaler.inverse_transform(y_pred_lstm)\n",
        "y_test = scaler.inverse_transform(y_test_lstm.reshape(-1, 1))"
      ],
      "metadata": {
        "id": "e72gDYQZ05oP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#_, y_test2 = create_sequences(test_data['unit_sales_scaled'].values, seq_length)\n",
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(test_data['date'][seq_length:], y_test, label=\"Actual Sales\")\n",
        "plt.plot(test_data['date'][seq_length:], y_pred_lstm, label=\"Predicted Sales\", linestyle=\"dashed\")\n",
        "plt.xlabel(\"Date\")\n",
        "plt.ylabel(\"Unit Sales\")\n",
        "plt.title(\"LSTM Forecasting for Unit Sales\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "53vFErhgsEnD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "with mlflow.start_run(run_name=\"Best LSTM Model\"):\n",
        "\n",
        "  for layer in lstm_model.layers:\n",
        "        if hasattr(layer, \"units\"):\n",
        "            mlflow.log_param(f\"{layer.name}_units\", layer.units)\n",
        "        if hasattr(layer, \"activation\"):\n",
        "            mlflow.log_param(f\"{layer.name}_activation\", layer.activation)\n",
        "\n",
        "  mse = mean_squared_error(y_test_lstm, y_pred_lstm)\n",
        "  rmse = np.sqrt (mse)\n",
        "  mlflow.log_metric(\"MSE\", mse)\n",
        "  mlflow.log_metric(\"RMSE\", rmse)\n",
        "\n",
        "with open('lstm_model.pkl', 'wb') as file:\n",
        "    pickle.dump(lstm_model, file)\n"
      ],
      "metadata": {
        "id": "js7yf4niZhG-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rmse"
      ],
      "metadata": {
        "id": "iaLTrCOihRvt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Compare LSTM vs. XGBoost"
      ],
      "metadata": {
        "id": "OVJ-4RLcKWnE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "rmse_xgb = mean_squared_error(y_test[seq_length:], y_pred_xgb, squared=False)\n",
        "rmse_lstm = rmse\n",
        "\n",
        "print(f'XGBoost RMSE: {rmse_xgb:.4f}')\n",
        "print(f'LSTM RMSE: {rmse_lstm:.4f}')"
      ],
      "metadata": {
        "id": "Zn-iKVJUKbXz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(14,6))\n",
        "\n",
        "plt.plot(test_data['date'][seq_length:], y_test, label=\"Actual Sales\", color=\"blue\", marker='o', linestyle='dashed')\n",
        "plt.plot(test_data['date'][seq_length:], y_pred, label=\"XGBoost Predictions\", color=\"red\", marker='s', linestyle='dashed')\n",
        "plt.plot(test_data['date'][seq_length:], y_pred_lstm, label=\"LSTM Predictions\", color=\"green\", marker='^', linestyle='dashed')\n",
        "\n",
        "# Formatting\n",
        "plt.xlabel(\"Date\")\n",
        "plt.ylabel(\"Unit Sales\")\n",
        "plt.title(\"Actual vs. Predicted Sales (XGBoost vs LSTM)\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "HnHM3t3KKjFZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "zqaNkzeh4nZi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "xgb_model = xgb.XGBRegressor(objective='reg:squarederror', random_state=42)\n",
        "\n",
        "param_dist = {\n",
        "    'n_estimators': np.arange(100, 1000, 100),\n",
        "    'max_depth': np.arange(3, 12, 1),\n",
        "    'learning_rate': np.linspace(0.01, 0.3, 10),\n",
        "    'subsample': np.linspace(0.5, 1.0, 5),\n",
        "    'colsample_bytree': np.linspace(0.5, 1.0, 5),\n",
        "    'gamma': np.linspace(0, 0.5, 5),\n",
        "    'reg_alpha': np.logspace(-3, 1, 5),\n",
        "    'reg_lambda': np.logspace(-3, 1, 5)\n",
        "}\n",
        "\n",
        "\n",
        "random_search = RandomizedSearchCV(\n",
        "    estimator=xgb_model,\n",
        "    param_distributions=param_dist,\n",
        "    n_iter=25,\n",
        "    scoring='neg_mean_squared_error',\n",
        "    cv=3,  #\n",
        "    verbose=2,\n",
        "    n_jobs=-1,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best hyperparameters:\", random_search.best_params_)"
      ],
      "metadata": {
        "id": "7J1GGM3E4rEW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "best_params = random_search.best_params_\n",
        "\n",
        "# Train the final XGBoost model with the best parameters\n",
        "final_xgb_model = xgb.XGBRegressor(**best_params, objective='reg:squarederror', random_state=42)\n",
        "final_xgb_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = final_xgb_model.predict(X_test)\n",
        "\n",
        "# Evaluate performance\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "rmse = np.sqrt(mse)\n",
        "\n",
        "print(f\"Final Model RMSE: {rmse:.4f}\")\n"
      ],
      "metadata": {
        "id": "S6HjOQwG41_2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "sns.scatterplot(x=y_test, y=y_pred, alpha=0.6)\n",
        "plt.xlabel(\"Actual Unit Sales\")\n",
        "plt.ylabel(\"Predicted Unit Sales\")\n",
        "plt.title(\"Actual vs. Predicted Unit Sales\")\n",
        "plt.axline([0, 0], slope=1, color='red', linestyle=\"--\")  # Perfect prediction line\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "eFRkfGJw44CC"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "private_outputs": true,
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}